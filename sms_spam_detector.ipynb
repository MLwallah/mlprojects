{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY0PL2_g3cg1"
   },
   "source": [
    "# **Spam Detector**\n",
    "#### Creating a simple spam identifier with techniques from Text Preprocessing and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cksLNeEVzuuO"
   },
   "source": [
    "## **Context**\n",
    "**Many users worldwide rely on email and text messages** for communication, and these have become an essential part of daily life. \n",
    "\n",
    "Emails and text messages, due to their importance to people's lives worldwide, may also however contain sensitive information that hackers could attempt to steal (data theft). As a result, it is often critical for email service providers to be able to distinguish between spam and ham (not spam) emails, to reduce the likelihood of a user suffering a data theft attack.\n",
    "\n",
    "Genuine email/text that is important to the user and informative, is referred to as ham. Spam, on the other hand, is bogus email/text sent from untrustworthy sources with malicious intent.\n",
    "\n",
    "## **Data Dictionary**\n",
    "* **Text:** Messages sent by the users\n",
    "* **Type:** Target variable which provides the label about a message being Spam or Ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebYh83Ca_OSR"
   },
   "source": [
    "Due to the presence of the **Type** label, this is of course just a **Supervised Learning Classification** problem of classifying if the Text is Spam or Ham. \n",
    "\n",
    "The catch is that **we don't explicitly have features (columns) in our data** - we only have the text of each message, and we have to attempt to **\"extract\"** numerical features from this text to feed into the Machine Learning algorithms we have already learnt before, to see if that is able to identify Ham vs Spam messages. \n",
    "\n",
    "As we will see in the rest of this notebook, the text preprocessing methods we have learnt so far, do in fact give us an initial level of success in this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "O0TtRkCe7JIh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in /Users/manishsingh/opt/anaconda3/lib/python3.9/site-packages (1.2.0)\n",
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[K     |████████████████████████████████| 622 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=4c32104fef6df84bb0bc71d2bf543811cbb83056c6cf30d96c5f967b4e9154e7\n",
      "  Stored in directory: /Users/manishsingh/Library/Caches/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Unidecode\n",
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn8jgFdvLWXh"
   },
   "source": [
    "## **Importing the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15818,
     "status": "ok",
     "timestamp": 1662369241945,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "fZBAuHBvD4HH",
    "outputId": "aeb16a27-13fd-4e61-ced1-ebfcebce0519"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 60]\n",
      "[nltk_data]     Operation timed out>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 60] Operation\n",
      "[nltk_data]     timed out>\n",
      "2023-05-17 19:01:51.210012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required the libraries\n",
    "\n",
    "# To read and manipulate the data\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# To visualise the graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Helps to display the images\n",
    "from PIL import Image\n",
    "\n",
    "# Helps to extract the data using regular expressions\n",
    "import re\n",
    "\n",
    "# Helps to remove the punctuation\n",
    "import string\n",
    "\n",
    "# It helps to remove the accented characters \n",
    "\n",
    "import unidecode\n",
    "\n",
    "# It help to correct the spellings\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "# Importing the NLTK library\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')    # Loading the stopwords\n",
    "nltk.download('punkt')        # Loading the punkt module, used in Tokenization\n",
    "nltk.download('omw-1.4')      # Dependency for Tokenization\n",
    "nltk.download('wordnet')      # Loading the wordnet module, used in stemming and lemmatization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Helps to visualize the wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Used in Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Used in Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Used in Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Importing the SpaCy library \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # Loading the envrionment config\n",
    "\n",
    "# Used in tokenization\n",
    "from spacy.lang.en import English\n",
    "en_nlp = English()\n",
    "\n",
    "# Helped to create train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Metrics to evaluate the model\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4dYhWtG0EIL"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3948,
     "status": "ok",
     "timestamp": 1662369289632,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "m5lDSLO3zrMy",
    "outputId": "4aac377c-95de-48fe-ad56-62037c1f5889"
   },
   "outputs": [],
   "source": [
    "# mounting the google drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Loading the data\n",
    "messages = pd.read_csv(\"sms_spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s4jz3_A-7w7H"
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the data frame\n",
    "data = messages.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUdVs6oqcAT8"
   },
   "source": [
    "## **Overview of the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaRznU-2crhy"
   },
   "source": [
    "### **View the first and last 5 rows of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1662369365509,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "xQf0NcvjzrMz",
    "outputId": "bcceba96-644d-4de7-a7a1-938c11181354"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in ñó ñó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..going bacckk to stävänger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also dong in cbe ony. Bt have to pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are you this much buzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Please ask mummy to call father</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                                     text\n",
       "0  ham  Hope you are having a good week. Just checking in ñó ñó\n",
       "1  ham                             K..going bacckk to stävänger\n",
       "2  ham                  Am also dong in cbe ony. Bt have to pay\n",
       "3  ham                                   Are you this much buzy\n",
       "4  ham                          Please ask mummy to call father"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1699,
     "status": "ok",
     "timestamp": 1662369372705,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "ybHg2f23zrM0",
    "outputId": "78e69498-9afb-4dbd-e356-4a1d9fbb0563"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>spam</td>\n",
       "      <td>Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country Liverpool played in mid week? Txt ansr to 82277. £1.50 SP:Tyrone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5056</th>\n",
       "      <td>spam</td>\n",
       "      <td>HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ur balance is now £500. Ur next question is: Who sang 'Uptown Girl' in the 80's ? 2 answer txt ur ANSWER to 83600. Good luck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>spam</td>\n",
       "      <td>If you don't, your prize will go to another customer. T&amp;C at www.t-c.biz 18+ 150p/min Polo Ltd Suite 373 London W1J 6HL Please call back if busy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>spam</td>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type  \\\n",
       "5055  spam   \n",
       "5056  spam   \n",
       "5057  spam   \n",
       "5058  spam   \n",
       "5059  spam   \n",
       "\n",
       "                                                                                                                                                            text  \n",
       "5055                    Sunshine Quiz Wkly Q! Win a top Sony DVD player if u know which country Liverpool played in mid week? Txt ansr to 82277. £1.50 SP:Tyrone  \n",
       "5056                                  HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call  \n",
       "5057                               Ur balance is now £500. Ur next question is: Who sang 'Uptown Girl' in the 80's ? 2 answer txt ur ANSWER to 83600. Good luck!  \n",
       "5058           If you don't, your prize will go to another customer. T&C at www.t-c.biz 18+ 150p/min Polo Ltd Suite 373 London W1J 6HL Please call back if busy   \n",
       "5059  SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN_AbO3kc0Jx"
   },
   "source": [
    "### **Understand the shape of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1662369378809,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "bU38HG7Kc1Bu",
    "outputId": "46b08ed1-ee59-49a9-efa9-6f53f352cdad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5060, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA2TiZB8c5ZJ"
   },
   "source": [
    "* The dataset has 5060 rows and 2 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq_gyYm0jMxN"
   },
   "source": [
    "### **Check the data types of the columns for the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1662369393004,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "XdfjLAzpjNdo",
    "outputId": "a7ffd250-b08d-4a27-9073-41bec4aabcc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5060 entries, 0 to 5059\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    5060 non-null   object\n",
      " 1   text    5060 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 79.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPs_UQPcjQ5D"
   },
   "source": [
    "* Both the columns are of the object type.\n",
    "* There are no null values in the dataset, so no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIDPOAnjgO3z"
   },
   "source": [
    "### **Checking for duplicate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1662369396897,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "eAdVjJkFdDjB",
    "outputId": "5c5718bf-5c7c-4d3b-de2b-921f85fb1a3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for duplicate values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1292,
     "status": "ok",
     "timestamp": 1662369402747,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "mO46NhqqgR-D",
    "outputId": "1f87f232-fc28-4521-f067-bc63cf1971d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the duplicates\n",
    "data = data.drop_duplicates(keep = 'first')\n",
    "\n",
    "# checking for duplicate values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FNJKsaueF9zp"
   },
   "outputs": [],
   "source": [
    "# resetting the index of the dataframe\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzVMHiv8hCC3"
   },
   "source": [
    "* The duplicate values have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHn-ONC-vIdN"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1662369411644,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "DhQBmnZVzrM0",
    "outputId": "91c2821c-b125-4797-c49d-4c168ad21a82"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ham     0.860381\n",
      "spam    0.139619\n",
      "Name: type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the distribution of ham and spam messages\n",
    "sns.countplot(x = 'type', data = data)\n",
    "plt.show()\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# checking the percentage of ham and spam messages\n",
    "print(data['type'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tYQN4N_voO0"
   },
   "source": [
    "* 86% of the messages in the dataset are ham, so **only around 14% of the messages are actually spam.**\n",
    "* This dataset is hence highly imbalanced.\n",
    "\n",
    "This is quite reflective of the use cases where Machine Learning is generally required, even in the NLP space. **In general, the event / occurrence we're trying to identify is rare**, and that's the value of Machine Learning and AI - being able to detect that needle in the haystack; in this case detecting the one spam message from a crowd of non-spam, which would obviously be critical to the utility of a good spam detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oFgqZA1KOYr"
   },
   "source": [
    "### **Let's now take a look at some of the messages in the data to get a sense for the text preprocessing required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1662369486652,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "IYUZBv7W01k4",
    "outputId": "9a0afe72-056a-4482-b6c0-c2745a786093"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Sure thing big man. i have hockey elections at 6, shouldn€˜t go on longer than an hour though     '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1662369454631,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "IFjahQ0H01or",
    "outputId": "c284c67e-2854-4f60-d612-2af49fa10fb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hope you are having a good week. Just checking in ñó ñó'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1614,
     "status": "ok",
     "timestamp": 1662369586871,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "p1MqZofK1dYz",
    "outputId": "257ba815-359e-4fb8-9ae2-8937d0369c6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050001808 from land line. Claim M95. Valid12hrs only'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][4320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1662369655227,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "W7-Vi6UOD6lt",
    "outputId": "d49f3fc8-e113-4993-c58c-dd81e18109e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for your Ringtone Order, Reference T91. You will be charged GBP 4 per week. You can unsubscribe at anytime by calling customer services on 09057039994'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][4330]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdkf2Yz37SFF"
   },
   "source": [
    "**From the random samples seen above, we observe that the text has:**\n",
    "\n",
    "*  Extra spaces\n",
    "\n",
    "*  Accented and Special characters\n",
    "\n",
    "*  Misspelled words\n",
    "\n",
    "*  Word inflections - can be observed in row 4320 where words like awarded and guaranteed can be reduced to their base forms.\n",
    "\n",
    "*  Lowercase and Uppercase characters\n",
    "\n",
    "\n",
    "**The presence of such unwanted text doesn't add any value and also impacts the learning of a model**, thus reducing the model's performance. \n",
    "\n",
    "Hence, we shall be removing such unwanted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp8jFaqK_88_"
   },
   "source": [
    "### **Let's first build a Random Forest model without performing any text preprocessing and observe the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8-behlVT-Mcw"
   },
   "outputs": [],
   "source": [
    "uncleaned_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lEzAutOi-OsQ"
   },
   "outputs": [],
   "source": [
    "X = uncleaned_data[\"text\"]\n",
    "Y = uncleaned_data[\"type\"].map({'ham':0,'spam':1})\n",
    "# encoding the features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Splitting data in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 1,stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#  in mca. But not conform.</th>\n",
       "      <th>#  mins but i had to stop somewhere first.</th>\n",
       "      <th>DECIMAL m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla</th>\n",
       "      <th>and  picking them up from various points</th>\n",
       "      <th>came to look at the flat, seems ok, in his 50s? * Is away alot wiv work. Got woman coming at 6.30 too.</th>\n",
       "      <th>gonna let me know cos comes bak from holiday that day.  is coming. Don't4get2text me  number.</th>\n",
       "      <th>said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day!</th>\n",
       "      <th>says that he's quitting at least5times a day so i wudn't take much notice of that. Nah, she didn't mind. Are you gonna see him again? Do you want to come to taunton tonight? U can tell me all about !</th>\n",
       "      <th>what number do u live at? Is it 11?</th>\n",
       "      <th>#  am I think? Should say on syllabus</th>\n",
       "      <th>...</th>\n",
       "      <th>whatever, im pretty pissed off.</th>\n",
       "      <th>when you and derek done with class?</th>\n",
       "      <th>will you like to be spoiled? :)</th>\n",
       "      <th>wiskey Brandy Rum Gin Beer Vodka Scotch Shampain Wine KUDIyarasu dhina vaazhthukkal. ..</th>\n",
       "      <th>would u believe it they didnt know i had thurs pre booked off so they re cancelled me AGAIN! that needs to b sacked</th>\n",
       "      <th>wow. You're right! I didn't mean to do that. I guess once i gave up on boston men and changed my search location to nyc, something changed. Cuz on my signin page it still says boston.</th>\n",
       "      <th>yay! finally lol. i missed our cinema trip last week :-(</th>\n",
       "      <th>yeah sure thing mate haunt got all my stuff sorted but im going sound anyway promoting hex for .by the way who is this? dont know number. Joke</th>\n",
       "      <th>yeah, that's what I was thinking</th>\n",
       "      <th>you are sweet as well, princess. Please tell me your likes and dislikes in bed...</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4676 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    #  in mca. But not conform.   #  mins but i had to stop somewhere first.  \\\n",
       "0                             0                                            0   \n",
       "1                             0                                            0   \n",
       "2                             0                                            0   \n",
       "3                             0                                            0   \n",
       "4                             0                                            0   \n",
       "\n",
       "    DECIMAL m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla  \\\n",
       "0                                                                                                                             0   \n",
       "1                                                                                                                             0   \n",
       "2                                                                                                                             0   \n",
       "3                                                                                                                             0   \n",
       "4                                                                                                                             0   \n",
       "\n",
       "    and  picking them up from various points  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "\n",
       "    came to look at the flat, seems ok, in his 50s? * Is away alot wiv work. Got woman coming at 6.30 too.  \\\n",
       "0                                                                                                        0   \n",
       "1                                                                                                        0   \n",
       "2                                                                                                        0   \n",
       "3                                                                                                        0   \n",
       "4                                                                                                        0   \n",
       "\n",
       "    gonna let me know cos comes bak from holiday that day.  is coming. Don't4get2text me  number.   \\\n",
       "0                                                                                                0   \n",
       "1                                                                                                0   \n",
       "2                                                                                                0   \n",
       "3                                                                                                0   \n",
       "4                                                                                                0   \n",
       "\n",
       "    said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day!   \\\n",
       "0                                                                                                                                      0   \n",
       "1                                                                                                                                      0   \n",
       "2                                                                                                                                      0   \n",
       "3                                                                                                                                      0   \n",
       "4                                                                                                                                      0   \n",
       "\n",
       "    says that he's quitting at least5times a day so i wudn't take much notice of that. Nah, she didn't mind. Are you gonna see him again? Do you want to come to taunton tonight? U can tell me all about !  \\\n",
       "0                                                                                                                                                                                                         0   \n",
       "1                                                                                                                                                                                                         0   \n",
       "2                                                                                                                                                                                                         0   \n",
       "3                                                                                                                                                                                                         0   \n",
       "4                                                                                                                                                                                                         0   \n",
       "\n",
       "    what number do u live at? Is it 11?  \\\n",
       "0                                     0   \n",
       "1                                     0   \n",
       "2                                     0   \n",
       "3                                     0   \n",
       "4                                     0   \n",
       "\n",
       "   #  am I think? Should say on syllabus  ...  \\\n",
       "0                                      0  ...   \n",
       "1                                      0  ...   \n",
       "2                                      0  ...   \n",
       "3                                      0  ...   \n",
       "4                                      0  ...   \n",
       "\n",
       "   whatever, im pretty pissed off.  when you and derek done with class?  \\\n",
       "0                                0                                    0   \n",
       "1                                0                                    0   \n",
       "2                                0                                    0   \n",
       "3                                0                                    0   \n",
       "4                                0                                    0   \n",
       "\n",
       "   will you like to be spoiled? :)  \\\n",
       "0                                0   \n",
       "1                                0   \n",
       "2                                0   \n",
       "3                                0   \n",
       "4                                0   \n",
       "\n",
       "   wiskey Brandy Rum Gin Beer Vodka Scotch Shampain Wine KUDIyarasu dhina vaazhthukkal. ..  \\\n",
       "0                                                                                        0   \n",
       "1                                                                                        0   \n",
       "2                                                                                        0   \n",
       "3                                                                                        0   \n",
       "4                                                                                        0   \n",
       "\n",
       "   would u believe it they didnt know i had thurs pre booked off so they re cancelled me AGAIN! that needs to b sacked  \\\n",
       "0                                                                                                                    0   \n",
       "1                                                                                                                    0   \n",
       "2                                                                                                                    0   \n",
       "3                                                                                                                    0   \n",
       "4                                                                                                                    0   \n",
       "\n",
       "   wow. You're right! I didn't mean to do that. I guess once i gave up on boston men and changed my search location to nyc, something changed. Cuz on my signin page it still says boston.  \\\n",
       "0                                                                                                                                                                                        0   \n",
       "1                                                                                                                                                                                        0   \n",
       "2                                                                                                                                                                                        0   \n",
       "3                                                                                                                                                                                        0   \n",
       "4                                                                                                                                                                                        0   \n",
       "\n",
       "   yay! finally lol. i missed our cinema trip last week :-(  \\\n",
       "0                                                         0   \n",
       "1                                                         0   \n",
       "2                                                         0   \n",
       "3                                                         0   \n",
       "4                                                         0   \n",
       "\n",
       "   yeah sure thing mate haunt got all my stuff sorted but im going sound anyway promoting hex for .by the way who is this? dont know number. Joke  \\\n",
       "0                                                                                                                                               0   \n",
       "1                                                                                                                                               0   \n",
       "2                                                                                                                                               0   \n",
       "3                                                                                                                                               0   \n",
       "4                                                                                                                                               0   \n",
       "\n",
       "   yeah, that's what I was thinking  \\\n",
       "0                                 0   \n",
       "1                                 0   \n",
       "2                                 0   \n",
       "3                                 0   \n",
       "4                                 0   \n",
       "\n",
       "   you are sweet as well, princess. Please tell me your likes and dislikes in bed...  \n",
       "0                                                                                  0  \n",
       "1                                                                                  0  \n",
       "2                                                                                  0  \n",
       "3                                                                                  0  \n",
       "4                                                                                  0  \n",
       "\n",
       "[5 rows x 4676 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1662369669283,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "9IGn_7EOAn7q",
    "outputId": "e7d40caa-cf00-4533-cc64-0ba30cf22dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training set :  (3741, 4676)\n",
      "Shape of test set :  (936, 4676)\n",
      "Percentage of classes in training set:\n",
      "0    0.860465\n",
      "1    0.139535\n",
      "Name: type, dtype: float64\n",
      "Percentage of classes in test set:\n",
      "0    0.860043\n",
      "1    0.139957\n",
      "Name: type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "print(\"Percentage of classes in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Percentage of classes in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18409,
     "status": "ok",
     "timestamp": 1662369691219,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "uU3uBaL8_Azs",
    "outputId": "c6fcaed9-1009-406b-c67c-1bcca4014845"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intializing the Random Forest model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# fitting the model on training set\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1262,
     "status": "ok",
     "timestamp": 1662369695770,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "Oxpa96wf_K7O",
    "outputId": "52a3d66c-57ac-42a7-c3b6-4b3186607d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92       936\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86       936\n",
      "   macro avg       0.50      0.43      0.46       936\n",
      "weighted avg       1.00      0.86      0.92       936\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manishsingh/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/manishsingh/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/manishsingh/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# checking the metrics\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxARxtSnBpnT"
   },
   "source": [
    "- The model (even a fairly strong model such as the Random Forest) does not seem to be able to differentiate between ham and spam messages, and is simply predicting everything to be ham.\n",
    "- This appears to be a major drawback of using unclean text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA4wY9c2CEIs"
   },
   "source": [
    "\n",
    "**Now that we know performance on unclean data is very poor, let's clean the data with text preprocessing techniques** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MCckaY6_IOx"
   },
   "source": [
    "## **Text Preprocessing Step 1**\n",
    "## **Removing accented characters from the text**\n",
    "\n",
    "* Accents are special string characters generally adapted from other languages. They are not considered a major part of English.\n",
    "* By using the unidecode library, we can transliterate any unicode string into the closest possible representation in ASCII text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1662369837783,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "I9rSY4Ox_Hne",
    "outputId": "48bfe9fc-fdce-45c0-fcee-25e78ed0d3ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hope you are having a good week. Just checking in ñó ñó\n",
       "1                               K..going bacckk to stävänger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first two text messages\n",
    "data['text'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V93mRJII_ebu"
   },
   "source": [
    "We can see above that 'no no' and 'stavanger' are words with accented characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UbjzzHEy_Ywq"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "    t = data['text'][i].split()\n",
    "    # Applying the unicode data on a sentence and replacing the accented words to get the closest possible ASCII text\n",
    "    new_text = [unidecode.unidecode(word) for word in t]\n",
    "    \n",
    "    # Combining the seperate words into sentence\n",
    "    new_text = ' '.join(new_text)\n",
    "    # append the new_text to list text\n",
    "    text.append(new_text)\n",
    "\n",
    "data['cleaned_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1662369839195,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "WOwCHUW1DDik",
    "outputId": "695b5662-56d7-4203-bb7b-c74bc1ad46a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hope you are having a good week. Just checking in ñó ñó</td>\n",
       "      <td>Hope you are having a good week. Just checking in no no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K..going bacckk to stävänger</td>\n",
       "      <td>K..going bacckk to stavanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am also dong in cbe ony. Bt have to pay</td>\n",
       "      <td>Am also dong in cbe ony. Bt have to pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are you this much buzy</td>\n",
       "      <td>Are you this much buzy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "0  Hope you are having a good week. Just checking in ñó ñó   \n",
       "1                             K..going bacckk to stävänger   \n",
       "2                  Am also dong in cbe ony. Bt have to pay   \n",
       "3                                   Are you this much buzy   \n",
       "\n",
       "                                              cleaned_text  \n",
       "0  Hope you are having a good week. Just checking in no no  \n",
       "1                             K..going bacckk to stavanger  \n",
       "2                  Am also dong in cbe ony. Bt have to pay  \n",
       "3                                   Are you this much buzy  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0:3,['text','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bIVm3JOB5IG"
   },
   "source": [
    "- We observe that the accented words are now converted to normal words i.e **stävänger** to **stavanger** and **ñó** to no.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1wNrsU7Y-AY"
   },
   "source": [
    "## **Text Preprocessing Step 2**\n",
    "## **Removing special characters from the text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuqfekhJX2RW"
   },
   "source": [
    "The function **isalnum()** method returns `True` if all the characters are alphanumeric, meaning alphabet letter (a-z) and numbers (0-9). By using this function we can retrive the data which consists of only letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1662369839196,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "6Dqg_aGNDMiw",
    "outputId": "cfc5976f-f25d-41ee-cfb4-dae3cafee904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yup bathe liao...'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's consider the below example\n",
    "data['text'][75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iJTVLDaDysR"
   },
   "source": [
    "* We can see that `liao...` has special symbols or extra punctuation in the text and this needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1662369839196,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "Ed-Omfs2CSw3",
    "outputId": "e21c602f-c981-4028-b303-71db0ee77912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yup', 'bathe', 'liao...']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting text into separate words\n",
    "example = data['text'][75].split()\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1662369839197,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "LzZ1c8CfCmEa",
    "outputId": "3555d713-2087-4465-8b9c-d775159683ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New list with only alphabets and numbers:  ['Yup', 'bathe']\n",
      "Original Text:  Yup bathe\n"
     ]
    }
   ],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each element of the list\n",
    "for i in example:\n",
    "  if i.isalnum():  # checking if the element is an alphabet or a number\n",
    "    new_text = i   # if the element of the list is a number assign it to variable new_text\n",
    "    text.append(new_text) # append the new_text to empty list text\n",
    "\n",
    "print('New list with only alphabets and numbers: ', text)\n",
    "\n",
    "# retriving the original text\n",
    "# join() is an inbuilt string function in Python takes all items in an iterable and joins them into one string. \n",
    "print('Original Text: ',' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9Xh5cSbFLuI"
   },
   "source": [
    "### **Now let's apply the above steps to all the messages in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ti4dw4xtYenu"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "    words = data['cleaned_text'][i].split() # splitting text into separate words\n",
    "    new_text = ' '.join([element for element in words if element.isalnum()]) # looping over each element of the list and using the join() function to retrive the original text\n",
    "    text.append(new_text)  # append the new_text to empty list text\n",
    "    \n",
    "data['cleaned_text'] = text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPIQwom4ZLgM"
   },
   "source": [
    "Let's take a look at some random samples to view the messages now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1662369839199,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "KtMkuqkafmp4",
    "outputId": "5365b56a-9170-42ee-8978-7e5b5b79db2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K..going bacckk to stävänger</td>\n",
       "      <td>bacckk to stavanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K I'll be sure to get up before noon and see what's what</td>\n",
       "      <td>K be sure to get up before noon and see what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Yup bathe liao...</td>\n",
       "      <td>Yup bathe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>I am getting threats from your sales executive Shifad as i raised complaint against him. Its an official message.</td>\n",
       "      <td>I am getting threats from your sales executive Shifad as i raised complaint against Its an official</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>S.this will increase the chance of winning.</td>\n",
       "      <td>will increase the chance of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>How are you doing. How's the queen. Are you going for the royal wedding</td>\n",
       "      <td>How are you the Are you going for the royal wedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "1                                                                                        K..going bacckk to stävänger   \n",
       "10                                                           K I'll be sure to get up before noon and see what's what   \n",
       "75                                                                                                  Yup bathe liao...   \n",
       "76  I am getting threats from your sales executive Shifad as i raised complaint against him. Its an official message.   \n",
       "77                                                                        S.this will increase the chance of winning.   \n",
       "78                                            How are you doing. How's the queen. Are you going for the royal wedding   \n",
       "\n",
       "                                                                                           cleaned_text  \n",
       "1                                                                                   bacckk to stavanger  \n",
       "10                                                         K be sure to get up before noon and see what  \n",
       "75                                                                                            Yup bathe  \n",
       "76  I am getting threats from your sales executive Shifad as i raised complaint against Its an official  \n",
       "77                                                                          will increase the chance of  \n",
       "78                                                  How are you the Are you going for the royal wedding  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[1,10,75,76,77,78],['text','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQBP3FLSMs_6"
   },
   "source": [
    "* **isalnum()** has removed the special characters from the text successfully.\n",
    "* If we observe the text closely, the words attached with the special characters have also been removed.\n",
    "* For example, in the text 'Yup bathe liao...' the word 'liao...' is removed as it was attached to special characters.\n",
    "* **isalnum()** is a stringent function and works well when the special characters are separated by a white space character or space.\n",
    "* Another efficient way to deal with special characters is to use the **regex** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HAbOefHgIIEk"
   },
   "outputs": [],
   "source": [
    "# importing regex library\n",
    "import re\n",
    "\n",
    "\n",
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# Specifying the pattern for regex to match a string of characters that are not a letters or numbers \n",
    "# regex will look for alphabets A to Z, a to z, and numbers 0 to 9\n",
    "pattern = '[^A-Za-z0-9]+'\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "    words = data['cleaned_text'][i].split() # splitting text into separate words\n",
    "    new_text = ''.join(re.sub(pattern, ' ', data['cleaned_text'][i])) # finding the pattern and using the join() function to retrive the original text\n",
    "    text.append(new_text) # append the new_text to empty list text\n",
    "    \n",
    "data['cleaned_text'] = text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08PKI_tp-mQV"
   },
   "source": [
    "Let's take a look at some random samples to view the text now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1662369839200,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "zw7vMP-fP16i",
    "outputId": "279bbaa2-24cc-480b-a2cd-249a389eef59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K..going bacckk to stävänger</td>\n",
       "      <td>bacckk to stavanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K I'll be sure to get up before noon and see what's what</td>\n",
       "      <td>K be sure to get up before noon and see what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Yup bathe liao...</td>\n",
       "      <td>Yup bathe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>I am getting threats from your sales executive Shifad as i raised complaint against him. Its an official message.</td>\n",
       "      <td>I am getting threats from your sales executive Shifad as i raised complaint against Its an official</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>S.this will increase the chance of winning.</td>\n",
       "      <td>will increase the chance of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>How are you doing. How's the queen. Are you going for the royal wedding</td>\n",
       "      <td>How are you the Are you going for the royal wedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "1                                                                                        K..going bacckk to stävänger   \n",
       "10                                                           K I'll be sure to get up before noon and see what's what   \n",
       "75                                                                                                  Yup bathe liao...   \n",
       "76  I am getting threats from your sales executive Shifad as i raised complaint against him. Its an official message.   \n",
       "77                                                                        S.this will increase the chance of winning.   \n",
       "78                                            How are you doing. How's the queen. Are you going for the royal wedding   \n",
       "\n",
       "                                                                                           cleaned_text  \n",
       "1                                                                                   bacckk to stavanger  \n",
       "10                                                         K be sure to get up before noon and see what  \n",
       "75                                                                                            Yup bathe  \n",
       "76  I am getting threats from your sales executive Shifad as i raised complaint against Its an official  \n",
       "77                                                                          will increase the chance of  \n",
       "78                                                  How are you the Are you going for the royal wedding  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[[1,10,75,76,77,78],['text','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "737OaaEGhf7C"
   },
   "source": [
    "- We can observe that regex simply removed the special characters and retained the alphabets and numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX3_fALPHSqL"
   },
   "source": [
    "## **Text Preprocessing Step 3**\n",
    "## **Lowercasing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhVVKf-NSGP9"
   },
   "source": [
    "* **Lowercasing is an important text preprocessing technique.** The goal is to change the input text's case so that the words \"text,\" \"Text,\" and \"TEXT\" are all treated equally. Strings in Python are of course case-sensitive, so Python will not automatically do this for us.\n",
    "* Lowercasing helps reduce duplication and obtain accurate counts - it is hence an important part of various **\"feature extraction\"** approaches for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iAiBsh7BcjA6"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "    # lowecasing the text using the lower() function\n",
    "    new_text = data['cleaned_text'][i].lower()\n",
    "    text.append(new_text) # append the new_text to empty list text\n",
    "\n",
    "data['cleaned_text'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sifuorQd_PSf"
   },
   "source": [
    "Let's take a look at some random samples now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1662369839201,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "-KzOskMHgVaB",
    "outputId": "c0d41b69-ece0-488c-f1f9-673fada71007"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050001808 from land line. Claim M95. Valid12hrs only</td>\n",
       "      <td>we are trying to contact todays draw shows that you have won a ps800 prize call 09050001808 from land claim valid12hrs only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>Bloomberg -Message center +447797706009 Why wait? Apply for your future http://careers. bloomberg.com</td>\n",
       "      <td>bloomberg center why apply for your future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>WIN a £200 Shopping spree every WEEK Starting NOW. 2 play text STORE to 88039. SkilGme. TsCs08714740323 1Winawk! age16 £1.50perweeksub.</td>\n",
       "      <td>win a ps200 shopping spree every week starting 2 play text store to tscs08714740323 age16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                              text  \\\n",
       "4320  URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050001808 from land line. Claim M95. Valid12hrs only   \n",
       "4321                                                         Bloomberg -Message center +447797706009 Why wait? Apply for your future http://careers. bloomberg.com   \n",
       "4322                       WIN a £200 Shopping spree every WEEK Starting NOW. 2 play text STORE to 88039. SkilGme. TsCs08714740323 1Winawk! age16 £1.50perweeksub.   \n",
       "\n",
       "                                                                                                                     cleaned_text  \n",
       "4320  we are trying to contact todays draw shows that you have won a ps800 prize call 09050001808 from land claim valid12hrs only  \n",
       "4321                                                                                   bloomberg center why apply for your future  \n",
       "4322                                    win a ps200 shopping spree every week starting 2 play text store to tscs08714740323 age16  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[4320:4322,['text','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_6t2BPsgkFL"
   },
   "source": [
    "- We can observe that, all the text has now successfully been converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZEkERIJH_Ox"
   },
   "source": [
    "## **Text Preprocessing Step 4**\n",
    "## **Stripping Extra Spaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcH6p24XzrM4"
   },
   "source": [
    "* Stripping helps remove spaces at the beginning and the end of the string/sentence. \n",
    "* The extra spaces in between the characters and the spaces at the start or end of the string do not add any value to the model, and will rather slow down its computation speed. \n",
    "\n",
    "Thus, we can remove these extra spaces to make the model more efficient and targeted during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7BitMyx9zrM4"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "    # removing extra spaces from the text using strip() function\n",
    "    new_text = data['cleaned_text'][i].strip() \n",
    "    text.append(new_text)  # append the new_text to empty list text\n",
    "    \n",
    "data['cleaned_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1662369839821,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "SuWM_LR0IVQ4",
    "outputId": "7ea645b9-f982-411a-cd4e-34096ac20381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stripping the text data:        Sure thing big man. i have hockey elections at 6, shouldn€˜t go on longer than an hour though     \n",
      "After stripping the text data: sure thing big i have hockey elections at go on longer than an hour though\n"
     ]
    }
   ],
   "source": [
    "print(\"Before stripping the text data:\",data['text'][6])\n",
    "print(\"After stripping the text data:\",data['cleaned_text'][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBBeMn1gIdaF"
   },
   "source": [
    "- We now see that we have eliminated redundant extra spaces from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs1gzEA9_A4"
   },
   "source": [
    "As we have now performed lowercasing, removed the special & accented characters and numbers, and also stripped the text of its extra spaces, **this would be a good time to correct misspelled words**, since the unwanted text has now been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC9Zb-4XzrM7"
   },
   "source": [
    "## **Text Preprocessing Step 5**\n",
    "## **Spellchecking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-FrtvegZzrM7"
   },
   "outputs": [],
   "source": [
    "#!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "btuKJVQmzrM7"
   },
   "outputs": [],
   "source": [
    "# intializing the spellchecker and setting the language to english\n",
    "spell = Speller(lang = 'en')\n",
    "\n",
    "# defining a function which will take text as an input break\n",
    "def autospell(text):\n",
    "  '''\n",
    "  A function which takes text as an input, performs a spell check, and autocorrects misspelled words  \n",
    "  text: text input (object)\n",
    "  '''\n",
    "  word = text.split() # splitting text into separate words \n",
    "  spells = [spell(w) for w in word] # Spell checking on each word\n",
    "  return \" \".join(spells) # retriving the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3Bbmv4gxzrM7"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "    # applying the autospell function on the text messages\n",
    "    new_text = autospell(data['cleaned_text'][i]) \n",
    "    text.append(new_text) # append the new_text to empty list text\n",
    "\n",
    "data['cleaned_text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662369944236,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "EaXHj1ZzpILj",
    "outputId": "67bdc463-409a-486a-a6ae-972251416c80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hope you are having a good week. Just checking in ñó ñó</td>\n",
       "      <td>hope you are having a good just checking in no no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K..going bacckk to stävänger</td>\n",
       "      <td>back to stranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am also dong in cbe ony. Bt have to pay</td>\n",
       "      <td>am also dong in cbe bt have to pay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "0  Hope you are having a good week. Just checking in ñó ñó   \n",
       "1                             K..going bacckk to stävänger   \n",
       "2                  Am also dong in cbe ony. Bt have to pay   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  hope you are having a good just checking in no no  \n",
       "1                                   back to stranger  \n",
       "2                 am also dong in cbe bt have to pay  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now have a look at the spell corrected words\n",
    "data.loc[0:2,['text','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZMBOFjezrM7"
   },
   "source": [
    "- We observe that the spell correction seems to have worked but is not at its best. \n",
    "- For row 1, 'bacckk' was corrected to back but it didn't work well for row 2 where many misspelled words were left as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQSR8NouibTS"
   },
   "source": [
    "## **Text Preprocessing Step 6**\n",
    "## **Stop word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVMaV8X6zrM8"
   },
   "source": [
    "* The simple idea with stop word removal is to **exclude words that appear frequently throughout** all the documents in the corpus. Pronouns and articles are typically categorized as stop words.\n",
    "\n",
    "* To implement this, we have two Python libraries that are built to be used for NLP operations. Let's have a look at them and we'll implement them through both libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj0TDlQbJaC_"
   },
   "source": [
    "**Before removing the stop words from the text however, let's have a look at the key words from the original data through visual representation.**\n",
    "\n",
    "### **Word Cloud**\n",
    "\n",
    "A word cloud (also known as a tag cloud or text cloud) is a **visual representation of text**, in which the words appear bigger the more often they are mentioned. Word clouds are great for visualizing unstructured text data and getting insights on trends and patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ9AVT18KNdd"
   },
   "source": [
    "**Let's look at the top 100 unique words in original messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 1584,
     "status": "ok",
     "timestamp": 1662369945799,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "s8nFUlNWKR0E",
    "outputId": "1ea78a6d-6999-410f-fad6-9bc2c0654b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 366171 words in the combination of all texts.\n"
     ]
    }
   ],
   "source": [
    "all_texts = \" \".join(texts for texts in data.text)\n",
    "print (\"There are {} words in the combination of all texts.\".format(len(all_texts)))\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "#wordcloud = WordCloud(max_font_size = 40, max_words=100, background_color=\"white\").generate(all_texts)\n",
    "#plt.figure(figsize=(8,12))\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0d2j2WGKKfV"
   },
   "source": [
    "**Let's look at the top 100 unique words in the messages where we have applied some text preprocessing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1662369947305,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "nSIuNonqh2jp",
    "outputId": "319a3c82-9c5d-4103-e1b7-b3c897d5ceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 274650 words in the combination of all texts.\n"
     ]
    }
   ],
   "source": [
    "all_texts = \" \".join(texts for texts in data.cleaned_text)\n",
    "print (\"There are {} words in the combination of all texts.\".format(len(all_texts)))\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "#wordcloud = WordCloud(max_font_size = 40, max_words=100, background_color=\"white\").generate(all_texts)\n",
    "#plt.figure(figsize=(8,12))\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtwq2AzlLVzW"
   },
   "source": [
    "- As we see, there were 366,171 words in the data before preprocessing and 274,650 words after the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38_s1hFyj007"
   },
   "source": [
    "### **Removing the stop words using the NLTK library**\n",
    "\n",
    "NLTK has an in-built list of stop words, and it can utilize that list to remove the stop words from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "IUEFcs57zrM8"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    word = data['cleaned_text'][i].split() # splitting text into separate words \n",
    "    # removing the english language stopwords from the messages\n",
    "    new_text = [w for w in word if not w in stopwords.words('english')] \n",
    "    new_text = ' '.join(new_text) #retriving the original text\n",
    "    text.append(new_text) # \n",
    "    \n",
    "data['NLTK_nostopwords_text'] = text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1662369953327,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "JSwPpU8SzrM8",
    "outputId": "e60b32c5-9055-48d1-9058-1b9edd4adfa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>NLTK_nostopwords_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sure thing big i have hockey elections at go on longer than an hour though</td>\n",
       "      <td>sure thing big hockey elections go longer hour though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i anything</td>\n",
       "      <td>anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>by march i should be but will call you for the problem is that my capital never how far with work and the ladies</td>\n",
       "      <td>march call problem capital never far work ladies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       cleaned_text  \\\n",
       "6                                        sure thing big i have hockey elections at go on longer than an hour though   \n",
       "7                                                                                                        i anything   \n",
       "8  by march i should be but will call you for the problem is that my capital never how far with work and the ladies   \n",
       "\n",
       "                                   NLTK_nostopwords_text  \n",
       "6  sure thing big hockey elections go longer hour though  \n",
       "7                                               anything  \n",
       "8       march call problem capital never far work ladies  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[6:8,['cleaned_text','NLTK_nostopwords_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM0T0T3-zrM9"
   },
   "source": [
    "- We observe that stop words are removed from the text column. \n",
    "- For row 6, `i have`, `at` and `than an` are removed. \n",
    "- In row 7, **i** character was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUtL9NGAMEEw"
   },
   "source": [
    "### **After stop words removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1662369953328,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "lcYsF_ppMIWw",
    "outputId": "3f6a56b1-b0e5-4893-d1c8-3dc6d13da012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 177931 words in the combination of all texts.\n"
     ]
    }
   ],
   "source": [
    "all_texts1 = \" \".join(texts for texts in data.NLTK_nostopwords_text)\n",
    "\n",
    "print (\"There are {} words in the combination of all texts.\".format(len(all_texts1)))\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "#wordcloud = WordCloud(max_font_size = 40, max_words=100, background_color=\"white\").generate(all_texts1)\n",
    "#plt.figure(figsize=(8,12))\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifo-DjO8BLpk"
   },
   "source": [
    "- We now have 177,931 words after the stop words removal from the cleaned_text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dtnsn4j-Brjw"
   },
   "source": [
    "### **Let's now implement the same with the spaCy library**\n",
    "\n",
    "Similar to NLTK, spaCy also has its list of stop words. Let's see how spaCy performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PVro_HfYBq8-"
   },
   "outputs": [],
   "source": [
    "# Loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# loading the stopwords\n",
    "spacy_stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "LhaqtuNSCtyb"
   },
   "outputs": [],
   "source": [
    "# defining an empty list\n",
    "text = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  t = data['cleaned_text'][i].split() # splitting text into separate words \n",
    "  # Removing the words that are present in the stopwords\n",
    "  words = [word for word in t if not word in spacy_stopwords] \n",
    "  words = ' '.join(words) #retriving the original text\n",
    "  text.append(words) # append the new_text to empty list text\n",
    "    \n",
    "data['spacy_nostopwords_text'] = text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biGRoCqqDJWA"
   },
   "source": [
    "**Let's have a comparison for both NLTK results and spaCy results to check how well they removed stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1662369954934,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "1jtWmSg5DAGL",
    "outputId": "61f00557-1ef2-4333-aef2-28e5a2859316"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>NLTK_nostopwords_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>i crashed out muddled on my</td>\n",
       "      <td>crashed muddled</td>\n",
       "      <td>crashed muddled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>dont show how put new pictures up on</td>\n",
       "      <td>dont new pictures</td>\n",
       "      <td>dont show put new pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>i grumpy old my mom was like you better not be then again i am always the one to play</td>\n",
       "      <td>grumpy old mom like better play</td>\n",
       "      <td>grumpy old mom like better always one play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>boo on my way to my making gorilla mmmm</td>\n",
       "      <td>boo way making gorilla mmmm</td>\n",
       "      <td>boo way making gorilla mmmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>what not under</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>so you think i should actually talk to not call his boss in the i went to this place last year and he told me where i could go and get my car fixed he kept telling me today how much he hoped i would come back how he always regretted not getting my</td>\n",
       "      <td>think actually talk boss went place year told car fixed kept telling today hoped come regretted getting</td>\n",
       "      <td>think actually talk call boss went place last year told could go get car fixed kept telling today much hoped would come back always regretted getting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                cleaned_text  \\\n",
       "100                                                                                                                                                                                                                              i crashed out muddled on my   \n",
       "101                                                                                                                                                                                                                     dont show how put new pictures up on   \n",
       "102                                                                                                                                                                    i grumpy old my mom was like you better not be then again i am always the one to play   \n",
       "103                                                                                                                                                                                                                  boo on my way to my making gorilla mmmm   \n",
       "104                                                                                                                                                                                                                                           what not under   \n",
       "105  so you think i should actually talk to not call his boss in the i went to this place last year and he told me where i could go and get my car fixed he kept telling me today how much he hoped i would come back how he always regretted not getting my   \n",
       "\n",
       "                                                                                      spacy_nostopwords_text  \\\n",
       "100                                                                                          crashed muddled   \n",
       "101                                                                                        dont new pictures   \n",
       "102                                                                          grumpy old mom like better play   \n",
       "103                                                                              boo way making gorilla mmmm   \n",
       "104                                                                                                            \n",
       "105  think actually talk boss went place year told car fixed kept telling today hoped come regretted getting   \n",
       "\n",
       "                                                                                                                                     NLTK_nostopwords_text  \n",
       "100                                                                                                                                        crashed muddled  \n",
       "101                                                                                                                             dont show put new pictures  \n",
       "102                                                                                                             grumpy old mom like better always one play  \n",
       "103                                                                                                                            boo way making gorilla mmmm  \n",
       "104                                                                                                                                                         \n",
       "105  think actually talk call boss went place last year told could go get car fixed kept telling today much hoped would come back always regretted getting  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[100:105,['cleaned_text','spacy_nostopwords_text','NLTK_nostopwords_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKA95oVGNlIA"
   },
   "source": [
    "From the above results, we can see that both libraries have performed decently and have been able to remove stop words. \n",
    "\n",
    "Let's look at how many words are left in the data after removing stop words using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiN-ObykN5jB"
   },
   "source": [
    "### **Results of spaCy through word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1662369954935,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "dsHXvEpbOCPS",
    "outputId": "fbbc58ad-9ca5-4f42-9190-605608f47492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156370 words in the combination of all texts.\n"
     ]
    }
   ],
   "source": [
    "all_texts2 = \" \".join(texts for texts in data.spacy_nostopwords_text)\n",
    "\n",
    "print (\"There are {} words in the combination of all texts.\".format(len(all_texts2)))\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "#wordcloud = WordCloud(max_font_size = 40, max_words = 200, background_color = \"white\").generate(all_texts2)\n",
    "#plt.figure(figsize = (8,12))\n",
    "\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMPHDtRxCJlI"
   },
   "source": [
    "- We got 156,370 after the stop word removal from the cleaned_text.\n",
    "- We can see that spaCy has been able to remove more stop words in comparison to NLTK.\n",
    "\n",
    "We'll use text output with no stop words given by spaCy for further steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v4ncbU0TeQH"
   },
   "source": [
    "Now that stop words have been removed and the text doesn't contain unnecessary characters or words. Let's now apply the **stemming** and **lemmatization** techniques to reduce words to their base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph5q5y4qmant"
   },
   "source": [
    "## **Text Preprocessing Step 7**\n",
    "## **Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nV5hoeSazrM-"
   },
   "source": [
    "We have three types of stemmers available in the NLTK library:\n",
    "\n",
    "- **Porter Stemmer**\n",
    "- **Snowball Stemmer**\n",
    "- **Lancaster Stemmer**\n",
    "\n",
    "**Note:** spaCy doesn't contain any function for stemming - it only provides support for lemmatization.\n",
    "\n",
    "We are going to store each stemming technique's results into separate columns, so that different models can be built on these outputs and we can observe which stemming method works the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvBTo90uzrM_"
   },
   "source": [
    "### **Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "wF6-6yvWzrM_"
   },
   "outputs": [],
   "source": [
    "# Initializing the Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# defining empty list for porter stemmer output\n",
    "porter_stems = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  text = data['spacy_nostopwords_text'][i].split() # splitting text into separate words \n",
    "  new_text = [ps.stem(word) for word in text] # applying the porter stemmer on every word of a mesaage\n",
    "  new_text = ' '.join(new_text) #retriving the original text\n",
    "  porter_stems.append(new_text) # append the text with base form of words in a message to empty list text\n",
    "    \n",
    "data['porter_text'] = porter_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1662369956107,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "WTgLryfqzrM_",
    "outputId": "a8d2f7ac-9ace-4ecc-d9ff-dbdce7491304"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>porter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>hey tmr maybe meet yck</td>\n",
       "      <td>hey tmr mayb meet yck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ok coming home</td>\n",
       "      <td>ok come home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>u called dad</td>\n",
       "      <td>u call dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>head text meet</td>\n",
       "      <td>head text meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>miss surprised gone net cafe miss</td>\n",
       "      <td>miss surpris gone net cafe miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               spacy_nostopwords_text                      porter_text\n",
       "70             hey tmr maybe meet yck            hey tmr mayb meet yck\n",
       "71                     ok coming home                     ok come home\n",
       "72                       u called dad                       u call dad\n",
       "73                     head text meet                   head text meet\n",
       "74  miss surprised gone net cafe miss  miss surpris gone net cafe miss"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[70:74,['spacy_nostopwords_text','porter_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syakFpj4zrM_"
   },
   "source": [
    "- If we observe the 70th and 73rd row, no word is converted into its root word since every word in the sentence already is in its base form.\n",
    "- We can observe that in the 71st row, the **coming** word is changed to **come**. In 72nd row **called** changed to **call**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YMAHY3UzrM_"
   },
   "source": [
    "### **Snowball Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xrP2qdWNzrM_"
   },
   "outputs": [],
   "source": [
    "# Initializing the Snowball Stemmer\n",
    "snowball = SnowballStemmer(language = 'english')\n",
    "\n",
    "# defining empty list for snowball stemmer output\n",
    "snow_stems = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  text = data['spacy_nostopwords_text'][i].split() # splitting text into separate words \n",
    "  new_text = [snowball.stem(word) for word in text]  # applying the snowball stemmer on every word of a mesaage\n",
    "  new_text = ' '.join(new_text) # retriving the original text\n",
    "  snow_stems.append(new_text) # append the text with base form of words in a message to empty list text\n",
    "    \n",
    "data['snowball_text'] = snow_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1662369956108,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "KWsLJupNzrM_",
    "outputId": "3b9960b8-c25b-4909-b2aa-794ed4a59146"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>snowball_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>hey tmr maybe meet yck</td>\n",
       "      <td>hey tmr mayb meet yck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ok coming home</td>\n",
       "      <td>ok come home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>u called dad</td>\n",
       "      <td>u call dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>head text meet</td>\n",
       "      <td>head text meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>miss surprised gone net cafe miss</td>\n",
       "      <td>miss surpris gone net cafe miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               spacy_nostopwords_text                    snowball_text\n",
       "70             hey tmr maybe meet yck            hey tmr mayb meet yck\n",
       "71                     ok coming home                     ok come home\n",
       "72                       u called dad                       u call dad\n",
       "73                     head text meet                   head text meet\n",
       "74  miss surprised gone net cafe miss  miss surpris gone net cafe miss"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[70:74,['spacy_nostopwords_text','snowball_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1HqjHg1zrM_"
   },
   "source": [
    "- The Snowball stemmer seems to have given similar results to the Porter stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N2ioYkLzrNA"
   },
   "source": [
    "### **Lancaster Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "XWyJfXIdzrNA"
   },
   "outputs": [],
   "source": [
    "# Initializing the Lancaster Stemmer\n",
    "lanc = LancasterStemmer()\n",
    "\n",
    "# defining empty list for lancaster stemmer output\n",
    "lanc_stems = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  text = data['spacy_nostopwords_text'][i].split() # splitting text into separate words \n",
    "  new_text = [lanc.stem(word) for word in text] # applying the lancaster stemmer on every word of a mesaage\n",
    "  new_text = ' '.join(new_text) # retriving the original text\n",
    "  lanc_stems.append(new_text) # append the text with base form of words in a message to empty list text\n",
    "    \n",
    "data['lancast_text'] = lanc_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1662369956109,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "-XfCXaRuzrNA",
    "outputId": "5d649abb-c723-43c2-dbad-8c373b5a8d1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>lancast_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>hey tmr maybe meet yck</td>\n",
       "      <td>hey tmr mayb meet yck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ok coming home</td>\n",
       "      <td>ok com hom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>u called dad</td>\n",
       "      <td>u cal dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>head text meet</td>\n",
       "      <td>head text meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>miss surprised gone net cafe miss</td>\n",
       "      <td>miss surpr gon net caf miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               spacy_nostopwords_text                 lancast_text\n",
       "70             hey tmr maybe meet yck        hey tmr mayb meet yck\n",
       "71                     ok coming home                   ok com hom\n",
       "72                       u called dad                    u cal dad\n",
       "73                     head text meet               head text meet\n",
       "74  miss surprised gone net cafe miss  miss surpr gon net caf miss"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[70:74,['spacy_nostopwords_text','lancast_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwkHlmSIzrNA"
   },
   "source": [
    "- The Lancaster stemmer results are a bit different - if we observe the 71st row, **coming** and **home** are transformed into **com** and **hom**. \n",
    "- In the 74th row **surprised** was translated to **surpr**, **gone** to **gon** and **cafe** to **caf**.\n",
    "\n",
    "\n",
    "The Lancaster stemmer being an aggresive stemmer in comparison to Porter and Snowball, has resulted in very different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrbSeMM3vqFr"
   },
   "source": [
    "**We have created the stem or root words for the text data using different stemming techniques. Now let's create lemmas for each word in the text using Lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkYOKl4MxLd3"
   },
   "source": [
    "## **Text Preprocessing Step 8**\n",
    "## **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EbEwglqzrNA"
   },
   "source": [
    "### **Lemmatization using the NLTK Library**\n",
    "\n",
    "Lemmatization is the transformation that uses a dictionary to map a word’s variant back to its root format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "z0MmkSIjzrNA"
   },
   "outputs": [],
   "source": [
    "# Implemenation using NLTK\n",
    "\n",
    "# initiating Lemmatizer available in NLTK library\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# defining empty list for lemmatizer output\n",
    "lemma = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  text = data['spacy_nostopwords_text'][i].split()  # splitting text into separate words \n",
    "  new_text = [lemmatizer.lemmatize(word) for word in text] # perfoming lemmatization on every word of a mesaage\n",
    "  new_text = ' '.join(new_text) # retriving the original text\n",
    "  lemma.append(new_text) # append the text with base form of words in a message to empty list text\n",
    "\n",
    "data['NLTK_lemmatized_text'] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1662372689825,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "dRNrTaY3zrNA",
    "outputId": "5a86b551-c270-46ac-d477-b237bb099d44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>NLTK_lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>think send love</td>\n",
       "      <td>think send love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>beautiful truth read heart feels light feels heavy leaves good night</td>\n",
       "      <td>beautiful truth read heart feel light feel heavy leaf good night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>love think hope day goes sleep miss long moment</td>\n",
       "      <td>love think hope day go sleep miss long moment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spacy_nostopwords_text  \\\n",
       "55                                                       think send love   \n",
       "56  beautiful truth read heart feels light feels heavy leaves good night   \n",
       "57                       love think hope day goes sleep miss long moment   \n",
       "\n",
       "                                                NLTK_lemmatized_text  \n",
       "55                                                   think send love  \n",
       "56  beautiful truth read heart feel light feel heavy leaf good night  \n",
       "57                     love think hope day go sleep miss long moment  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[55:57,['spacy_nostopwords_text','NLTK_lemmatized_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jSSo-0lWtw6"
   },
   "source": [
    "- In row 56, **leaves** were wrongly transformed into **leaf**, since the \"leaves\" word is being used here in the context of the verb \"leave\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWVyXyimXQjg"
   },
   "source": [
    "### **Lemmatization using the spaCy Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "jpNENFUJzrNB"
   },
   "outputs": [],
   "source": [
    "# Implemenation using SpaCy\n",
    "lemma_texts = []\n",
    "\n",
    "# looping over each message in the data\n",
    "for i in range(data.shape[0]):\n",
    "  #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "  doc = nlp(data['spacy_nostopwords_text'][i])\n",
    "  \n",
    "  # Create list of sentence tokens\n",
    "  lemma_list = []\n",
    "  \n",
    "  for token in doc:\n",
    "    lemma_list.append(token.lemma_)\n",
    "    lemma_text = ' '.join(lemma_list)\n",
    "  \n",
    "  lemma_texts.append(lemma_text)\n",
    "\n",
    "data['spacy_lemmatized_text'] = lemma_texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1662372662406,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "f66enr9izrNB",
    "outputId": "8d89d938-2a27-4ec6-9bd6-8faa3a442593"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_nostopwords_text</th>\n",
       "      <th>spacy_lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>think send love</td>\n",
       "      <td>think send love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>beautiful truth read heart feels light feels heavy leaves good night</td>\n",
       "      <td>beautiful truth read heart feel light feel heavy leave good night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>love think hope day goes sleep miss long moment</td>\n",
       "      <td>love think hope day goes sleep miss long moment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spacy_nostopwords_text  \\\n",
       "55                                                       think send love   \n",
       "56  beautiful truth read heart feels light feels heavy leaves good night   \n",
       "57                       love think hope day goes sleep miss long moment   \n",
       "\n",
       "                                                spacy_lemmatized_text  \n",
       "55                                                    think send love  \n",
       "56  beautiful truth read heart feel light feel heavy leave good night  \n",
       "57                    love think hope day goes sleep miss long moment  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[55:57,['spacy_nostopwords_text','spacy_lemmatized_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3ONtbUexdes"
   },
   "source": [
    "- In comparison to the NLTK library output for lemmatization, in row 56 spaCy was able to hold the context of the sentence. It transformed **leaves** to **leave** which was more suitable than **leaf**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiQjAmc9YboH"
   },
   "source": [
    "**We have now successfully cleaned the text data and transformed it into root words using stemming and lemmatization.** \n",
    "* In order to check which of these techniques worked best for us, we can build models with each one of them. \n",
    "* We can compare the performance of the model with the Porter, Snowball, and Lancaster stemmers to see which of the stemming algorithms helped us predict Ham vs Spam the best\n",
    "* Similarly, we can compare the model perfomance results of Stemming vs Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwuq1E_ayyKz"
   },
   "source": [
    "For the sake of this exercise and to see how text pre-processing has helped in improving the model performance, we will be using the text output received after removing the unnecessary rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8_A3XW0pQsd"
   },
   "source": [
    "## **Model Building**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "GZFZ32Kv7IXl"
   },
   "outputs": [],
   "source": [
    "cleaned_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "LPu1ngrS7IXm"
   },
   "outputs": [],
   "source": [
    "X = cleaned_data[\"NLTK_lemmatized_text\"]\n",
    "\n",
    "Y = cleaned_data[\"type\"].map({'ham':0,'spam':1})\n",
    "\n",
    "# encoding the features\n",
    "X = pd.get_dummies(X, drop_first = True)\n",
    "\n",
    "# Splitting data in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 1,stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1576,
     "status": "ok",
     "timestamp": 1662369990096,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "j7-d3end7IXm",
    "outputId": "cdd017a1-28ed-4343-ff2d-b633f35990ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training set :  (4209, 4330)\n",
      "Shape of test set :  (468, 4330)\n",
      "Percentage of classes in training set:\n",
      "0    0.860299\n",
      "1    0.139701\n",
      "Name: type, dtype: float64\n",
      "Percentage of classes in test set:\n",
      "0    0.861111\n",
      "1    0.138889\n",
      "Name: type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "\n",
    "print(\"Percentage of classes in training set:\")\n",
    "\n",
    "print(y_train.value_counts(normalize = True))\n",
    "\n",
    "print(\"Percentage of classes in test set:\")\n",
    "\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20643,
     "status": "ok",
     "timestamp": 1662370010733,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "G8pSjfSM7IXm",
    "outputId": "96863ff6-cede-4b0c-90a5-c3bcc87b8b9c"
   },
   "outputs": [],
   "source": [
    "# intializing the Random Forest model\n",
    "model = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "# fitting the model on training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1662370010734,
     "user": {
      "displayName": "Jai Ganesh Nagidi",
      "userId": "07935140246222778634"
     },
     "user_tz": -330
    },
    "id": "M1wgHSGV7IXm",
    "outputId": "99d2283e-a772-4753-b08c-c4ee05222523"
   },
   "outputs": [],
   "source": [
    "# making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# checking the metrics\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfc38k7J7IXo"
   },
   "source": [
    "- As we can see, the model is now able to differentiate between ham and spam messages to some degree, as the performance of the model for class 1 has improved.\n",
    "- The recall has improved, although the precision is not that great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ptmxdSD8iuu"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "* **We were hence able to establish the need for text preprocessing** by checking the performance of the model before and after preprocessing. It has clearly had some effect on improving our ability to identify spam messages.\n",
    "* **This notebook is however only a naïve introduction to the large world of NLP.** We took the liberty to apply one-hot encoding, one of the simplest possible methods, to the text-based dataset to convert it into numerical features. \n",
    "* **One-hot encoding, as we will learn soon, is not the ideal approach for encoding text data.** In future video lectures, we will discuss the problems with performing one-hot encoding on text, and how to encode text data in more intuitive ways such as Bag of Words and TF-IDF and also more recent, advanced dense embedding techniques."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8ptmxdSD8iuu"
   ],
   "provenance": [
    {
     "file_id": "1SbDEa78cFnQXqB3R8uag8ULSyaw7GBX-",
     "timestamp": 1662368962944
    },
    {
     "file_id": "1Cg4SwXRWx093WMBTX-ohxlH1-w1cvHEz",
     "timestamp": 1661931558503
    },
    {
     "file_id": "1SsVEMF1kC_aVfuV2Nxc-FHldkmeoGL02",
     "timestamp": 1661355324942
    },
    {
     "file_id": "1ZQKojIrNaOI14WHBKIsK3zJJLVRpziL5",
     "timestamp": 1660986982158
    },
    {
     "file_id": "1BSeLLRPJQb4NQKR3BNxEvtvEk1ISRZP0",
     "timestamp": 1660730567023
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
