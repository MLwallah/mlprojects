{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzOltfVmXHTjSY+N678ShR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLwallah/mlprojects/blob/main/nltk_spacy_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Library**"
      ],
      "metadata": {
        "id": "VD-M61Y2x9BF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k2xqEWm7rufG"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'punkt' module that will be helpful for tokenization\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L03veG7orzoN",
        "outputId": "5aeb195c-4ec0-443a-916a-b6c76871fcbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'stopwords' module that will be helpful for Stopwords removal\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oET0FzLtr6xx",
        "outputId": "ad9ae7d4-ca01-404d-beb1-53942537bca7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'wordnet' module that will be helpful for stemming and lemmatization\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeLvH3BgsEqI",
        "outputId": "16f0a55c-367a-4cbd-886f-fef8ececc756"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'omw-1.4' dependency for tokenization\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0urotUEksX1J",
        "outputId": "1558f5d5-9148-4c38-a6e8-f239da12c114"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'averaged_perceptron_tagger' for POS_Tagging\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3hoMwAds7uQ",
        "outputId": "0e6a57a7-236f-48d3-994e-fd2f914b54ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the required modules that are used in NER tagging\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn5x1Mwjt3ma",
        "outputId": "66ac225c-3e7b-43a0-bb95-467a58d13f77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all the necessary functions that are required to perform the tasks on text data using the downloaded modules."
      ],
      "metadata": {
        "id": "D-0GhYiIv5fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful to remove the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Helpful in Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Helpful in Tokenization\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Helpful in stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Used in NER tagging\n",
        "from nltk.corpus import treebank_chunk\n",
        "from nltk.chunk import ne_chunk"
      ],
      "metadata": {
        "id": "rAudYVlVwPYX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPACY library**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bkMlD1Yn0NJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfnHKmsSwcWC",
        "outputId": "bf5842b1-9f19-49d2-d8bc-ae9a5349dd70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the spacy library, we have an English language module that helps to perform language model operations on English text. "
      ],
      "metadata": {
        "id": "FS7L4kBjwcZP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the spacy language module\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8Jx-6nZwccU",
        "outputId": "f743735c-8ea0-4ca3-afb9-aac5e4670d9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-16 14:12:07.190526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the small english corpus\n",
        "spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6MpXK7Iwce9",
        "outputId": "32d950d6-63ac-4d17-e54b-98d116bebb41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f609cd105e0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similarly, we can download and load up other language models such as French language\n",
        "\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1OAWfAq4Jfp",
        "outputId": "63996f58-4a9b-4a29-e8c8-632a08c1913c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-16 14:12:24.985090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the French language corpus\n",
        "spacy.load('fr_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHSl7src4vix",
        "outputId": "7a0aab0b-df12-4997-861c-d727e5d0de89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.fr.French at 0x7f609b959300>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's now import the necessary functions from the spacy library that are required to perform tasks on text data using the downloaded modules"
      ],
      "metadata": {
        "id": "Lji691r946q0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()"
      ],
      "metadata": {
        "id": "6Ia44HYD6yUF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unidecode module → The unidecode module accepts unicode string values and returns a unicode string in Python3. By using the unidecode libraray, we can transliterate any unicode string into the closest possible representation in the ASCII text."
      ],
      "metadata": {
        "id": "nNw071227E5E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWUB1pXV8SA7",
        "outputId": "48e22e10-5e1c-484e-87ce-f842404eeceb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling check module → this will help correct the spellings of our text using Python library"
      ],
      "metadata": {
        "id": "O43q3ZBE8Xmj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUTfYh_m_b1h",
        "outputId": "7e85ddbe-9085-44ed-ad61-ca28c107fd6e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=a87e22091424c353827cf53be8521bbf292fa070ab8cd9397919e1820d7584aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply some text preprocessing and language model techniques on our text data using both the NLTK and spaCy libraries."
      ],
      "metadata": {
        "id": "QEuRpmZ0_32w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week'''\n"
      ],
      "metadata": {
        "id": "eu1gDYTw_gWS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's print text\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URSiMkTv_0Zb",
        "outputId": "6ba87c2d-7734-4686-89a2-0b0c07c25dbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English tokenizer, tagger, parser and Named Entity Recognition\n",
        "\n",
        "en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "iWK7c-PnAlYf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Holds the stopwords\n",
        "sp_stopwords = en.Defaults.stop_words"
      ],
      "metadata": {
        "id": "daoE8FACA8vl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the text data to nlp() parser and store the text into doc variable\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "TFsOmBWnBDoh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Word Removal\n",
        "\n",
        "Now first we shall remove the Stop words from our text data. Stop words are common words that are often omitted when text is analyzed. Both the NLTK & spaCy toolkits provide lists of stop words."
      ],
      "metadata": {
        "id": "tbuVv-ArC_bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK \n",
        "\n",
        "sorted(nltk.corpus.stopwords.words('english'))[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXY48oJjDAFK",
        "outputId": "be23d914-0143-458b-d1cc-5aa490babe9a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spacy\n",
        "\n",
        "sorted(sp_stopwords)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKmpNGUVD0Ru",
        "outputId": "88439d06-0b47-4fc2-e390-d8a72b935f1d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week'''\n"
      ],
      "metadata": {
        "id": "u4tCwNGpELyr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = text.split()\n",
        "\n",
        "# Removing the stopwords\n",
        "words = [word for word in data if not word in stopwords.words('english')]\n",
        "words = ' '.join(words)\n"
      ],
      "metadata": {
        "id": "eRHpZ7HoF7bh"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's find out the number of words present in the text data.\n",
        "print(\"No of words:\" , len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGQgcklbF-Rx",
        "outputId": "6fbc37f6-24ca-4d71-ce26-05646e094553"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of words: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's find out the number of words present in the text data after removing the stopwords.\n",
        "\n",
        "print(\"No of words:\" , len(words.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMsCpU2EGBEm",
        "outputId": "a1842886-f9fd-4caf-fb05-39e99a7e5662"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of words: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we had a total of 56 words, among them NLTK removed 18 stopwords from the text data, leaving us with 38 words remaining."
      ],
      "metadata": {
        "id": "-tzyfEPvSBUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's checkout words after using Spacy method"
      ],
      "metadata": {
        "id": "vddwl0IrS3Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = text.split()\n",
        "\n",
        "words = [word for word in data2 if not word in sp_stopwords]"
      ],
      "metadata": {
        "id": "_-dd8a8CSEvU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_length = len(words)"
      ],
      "metadata": {
        "id": "dYejcQ3iSevY"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's find out the number of words present in the text data after removing the stopwords.\n",
        "\n",
        "print(\"No of words:\" , sp_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNjp9MJrSk3n",
        "outputId": "c9aae7a5-ccff-493a-8408-1b7eda1c781c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of words: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy ends up removing just one more stop word than NLTK, leaving us with 37 words in the end. Both libraries hence seem to remove stop words in more or less a similar manner."
      ],
      "metadata": {
        "id": "W3-zsoQlTYFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "ZWMma6LjUEtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK\n",
        "\n",
        "result = word_tokenize(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tal7P94-UOVT",
        "outputId": "d0b8f629-0c9b-49c5-97e9-3ede45417574"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'Sebastian', 'Thrun', 'started', 'working', 'on', 'self-driving', 'cars', 'at', 'Google', 'in', '2007', ',', 'few', 'people', 'outside', 'of', 'the', 'company', 'took', 'him', 'seriously', '.', 'I', 'can', 'tell', 'you', 'very', 'senior', 'CEOs', 'of', 'major', 'American', 'car', 'companies', 'would', 'shake', 'my', 'hand', 'and', 'turn', 'away', 'because', 'I', 'wasn', '’', 't', 'worth', 'talking', 'to', ',', 'said', 'Thrun', ',', 'in', 'an', 'interview', 'with', 'Recode', 'earlier', 'this', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw3ogvLAUdHn",
        "outputId": "71d84e2b-dc78-4e72-fe3f-4a04223728cf"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, we got 62 word tokens for this text data.\n",
        "\n",
        "Now let's look at Sentence Tokenization and how it breaks the text down by sentence.\n",
        "\n",
        "We shall implement this with NLTK as well."
      ],
      "metadata": {
        "id": "uSkuwC6gVa_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = sent_tokenize(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg-du3z6Vd1A",
        "outputId": "e7dbead4-d8e6-40aa-b1f9-83dce2245a45"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.',\n",
              " 'I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \\nNumber of Sentence Tokens\", len(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYn-q0hfVpoT",
        "outputId": "1d4f7c40-bd70-4d27-b2c0-78db4f42c43c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Number of Sentence Tokens 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SPACY\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for token in doc:\n",
        "  tokens.append(token.text)\n",
        "\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3CLKyq2Vy4w",
        "outputId": "3bab3d7f-df2a-4ec1-98f3-6965e1b7e944"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'Sebastian', 'Thrun', 'started', 'working', 'on', 'self', '-', 'driving', 'cars', 'at', 'Google', 'in', '2007', ',', 'few', 'people', 'outside', 'of', 'the', 'company', 'took', 'him', 'seriously', '.', 'I', 'can', 'tell', 'you', 'very', 'senior', 'CEOs', 'of', 'major', 'American', 'car', 'companies', 'would', 'shake', 'my', 'hand', 'and', 'turn', 'away', 'because', 'I', 'was', 'n’t', 'worth', 'talking', 'to', ',', 'said', 'Thrun', ',', 'in', 'an', 'interview', 'with', 'Recode', 'earlier', 'this', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y18LucwXdk_",
        "outputId": "700b588b-7a88-4a0a-ddbe-0cf3ff847b05"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   When compared to the NLTK library, spaCy created 63 word tokens for the given text data.\n",
        "\n",
        "*   The difference appears to be with the hyphen character '-', NLTK did not consider that a separate token, but spaCy does.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JkA-MsdX6Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization using spaCy\n",
        "\n",
        "# Adding the pipeline 'sentencizer' component\n",
        "sbd = nlp.add_pipe('sentencizer')\n",
        "    \n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "content = nlp(text)\n",
        "\n",
        "# Create list of sentence tokens\n",
        "sents_list = []\n",
        "\n",
        "for sent in content.sents:\n",
        "        sents_list.append(sent.text)\n",
        "sents_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVFQyHcIYACc",
        "outputId": "26b1ebc4-a5a5-45f9-cfe2-8bdd20d3a236"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.',\n",
              " 'I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to, said Thrun, in an interview with Recode earlier this week']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   spaCy also creates the same two sentence tokens from the text data, the way NLTK does.\n",
        "\n"
      ],
      "metadata": {
        "id": "tiNBaHE7YpbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming**\n",
        "\n",
        "Stemming is the process of reducing a word to its stem or root format.\n",
        "\n",
        "Let us implement Stemming using the NLTK library."
      ],
      "metadata": {
        "id": "h9mki2J1eofD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating object for the PorterStemmer() class\n",
        "ps = PorterStemmer()\n",
        "\n",
        "porter_stems = []\n",
        "\n",
        "# Word Tokenization using NLTK Library\n",
        "token_data = word_tokenize(text)\n",
        "\n",
        "for word in token_data:\n",
        "    # Taking root word from actual word\n",
        "    result = ps.stem(word)\n",
        "    # Appending the root word into porter_stems list\n",
        "    porter_stems.append(result)"
      ],
      "metadata": {
        "id": "BOt0ABDweu3m"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzYHSXXXy5BD",
        "outputId": "2251b7e8-8da8-4eca-bec8-d73ba1895f58"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when', 'sebastian', 'thrun', 'start', 'work', 'on', 'self-driv', 'car', 'at', 'googl', 'in', '2007', ',', 'few', 'peopl', 'outsid', 'of', 'the', 'compani', 'took', 'him', 'serious', '.', 'i', 'can', 'tell', 'you', 'veri', 'senior', 'ceo', 'of', 'major', 'american', 'car', 'compani', 'would', 'shake', 'my', 'hand', 'and', 'turn', 'away', 'becaus', 'i', 'wasn', '’', 't', 'worth', 'talk', 'to', ',', 'said', 'thrun', ',', 'in', 'an', 'interview', 'with', 'recod', 'earlier', 'thi', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   As we can see, we have converted each token into its respective root word / stem. Let's create a dataframe and compare how the tokens get converted into their root words.\n",
        "*   Similar to how this was done with the Porter Stemmer, you can implement this for the other available stemming techniques, such as the Snowball Stemmer and the Lancaster Stemmer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pUrKYojUzCZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: Stemming techniques are not available for the spaCy library."
      ],
      "metadata": {
        "id": "sJr51cIdzLcZ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the data frame with actual token and its respective stemmed word\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'tokens':token_data,'stem_words':porter_stems})"
      ],
      "metadata": {
        "id": "38HrtTOjzQww"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sMcjgrXnzXj5",
        "outputId": "634ad342-534a-490d-b3a1-a24c68edf221"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      tokens stem_words\n",
              "0       When       when\n",
              "1  Sebastian  sebastian\n",
              "2      Thrun      thrun\n",
              "3    started      start\n",
              "4    working       work"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4929a198-2ffa-49fd-a890-da8a31e11bb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>stem_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When</td>\n",
              "      <td>when</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebastian</td>\n",
              "      <td>sebastian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thrun</td>\n",
              "      <td>thrun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>started</td>\n",
              "      <td>start</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>working</td>\n",
              "      <td>work</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4929a198-2ffa-49fd-a890-da8a31e11bb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4929a198-2ffa-49fd-a890-da8a31e11bb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4929a198-2ffa-49fd-a890-da8a31e11bb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**\n",
        "\n",
        "\n",
        "Lemmatization is similar to but subtly different from Stemming - it is the transformation that uses a dictionary to map a word’s variant back to its root format."
      ],
      "metadata": {
        "id": "L9hXIJxTzlRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implemenation using nltk\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemma = []\n",
        "\n",
        "# Word tokenization using NLTK Library\n",
        "token_data = word_tokenize(text)\n",
        "\n",
        "for word in token_data:\n",
        "    # Taking lemma word from actual word\n",
        "    result = lemmatizer.lemmatize(word)\n",
        "    # Appending the lemma into result list   \n",
        "    lemma.append(result)"
      ],
      "metadata": {
        "id": "zSpKpXRdznVI"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have converted each token into its respective root lemma. Let's create a DataFrame and compare how the tokens get converted into their root words."
      ],
      "metadata": {
        "id": "bObnzJL4z97t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the data frame with actual token and its respective lemma word for visibility\n",
        "new_df = pd.DataFrame({'tokens':token_data,'lemma_words':lemma})"
      ],
      "metadata": {
        "id": "m239-9zXz-1F"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NjDBajeP0CSO",
        "outputId": "6637aa5b-36ae-4936-9c5d-562a1c9604e0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     tokens lemma_words\n",
              "57     with        with\n",
              "58   Recode      Recode\n",
              "59  earlier     earlier\n",
              "60     this        this\n",
              "61     week        week"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd4d72dd-5fd2-4f6b-9c3c-36195521b2f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemma_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>with</td>\n",
              "      <td>with</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Recode</td>\n",
              "      <td>Recode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>earlier</td>\n",
              "      <td>earlier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>this</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>week</td>\n",
              "      <td>week</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd4d72dd-5fd2-4f6b-9c3c-36195521b2f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd4d72dd-5fd2-4f6b-9c3c-36195521b2f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd4d72dd-5fd2-4f6b-9c3c-36195521b2f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We observe that in these examples, there appears to be no change between the tokens and their lemma words, as the tokens already  appear to be in their root word format."
      ],
      "metadata": {
        "id": "StSb-L_90Fma"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy**"
      ],
      "metadata": {
        "id": "TUfbR8xx0Usf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the nlp pipeline to text, to get the desired output\n",
        "doc = en(text)"
      ],
      "metadata": {
        "id": "oGa4nuVl0YXE"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "spacy_lemma = []\n",
        "\n",
        "for word in doc:\n",
        "    # Appending tokens into tokens list\n",
        "    tokens.append(word.text)\n",
        "    # Storing the lemma into spacy_lemma list\n",
        "    spacy_lemma.append(word.lemma_)"
      ],
      "metadata": {
        "id": "340rUZFQ0ar5"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the data frame with actual token and its respective lemma word for visibility\n",
        "sp_df = pd.DataFrame({'tokens':tokens,'lemma_words':spacy_lemma})"
      ],
      "metadata": {
        "id": "6XWf5y9M0dXO"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rp7iBDzr0gyH",
        "outputId": "2d259cfa-f7e3-4a6c-f0ed-a4e96d19df75"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      tokens lemma_words\n",
              "0       When        when\n",
              "1  Sebastian   sebastian\n",
              "2      Thrun       Thrun\n",
              "3    started       start\n",
              "4    working        work"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa92afd2-fbca-4ddd-aa5a-f4bca59c4ce5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>lemma_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When</td>\n",
              "      <td>when</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebastian</td>\n",
              "      <td>sebastian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thrun</td>\n",
              "      <td>Thrun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>started</td>\n",
              "      <td>start</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>working</td>\n",
              "      <td>work</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa92afd2-fbca-4ddd-aa5a-f4bca59c4ce5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa92afd2-fbca-4ddd-aa5a-f4bca59c4ce5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa92afd2-fbca-4ddd-aa5a-f4bca59c4ce5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-speech Tagging (POS Tagging)\n",
        "\n",
        "Part-of-speech (POS) tagging is a popular Natural Language Processing method which refers to categorizing words in a text (corpus) in correspondence with a particular part-of-speech, depending on the definition of the word and its context.\n",
        "\n",
        "For example: The word **when** can be tagged as a subordinating conjunction."
      ],
      "metadata": {
        "id": "3buENthN0vvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK\n",
        "\n",
        "output = nltk.pos_tag(token_data)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ni0tp3800gw",
        "outputId": "d79287c5-3940-4adf-c8e3-910d395db426"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('When', 'WRB'),\n",
              " ('Sebastian', 'JJ'),\n",
              " ('Thrun', 'NNP'),\n",
              " ('started', 'VBD'),\n",
              " ('working', 'VBG'),\n",
              " ('on', 'IN'),\n",
              " ('self-driving', 'JJ'),\n",
              " ('cars', 'NNS'),\n",
              " ('at', 'IN'),\n",
              " ('Google', 'NNP'),\n",
              " ('in', 'IN'),\n",
              " ('2007', 'CD'),\n",
              " (',', ','),\n",
              " ('few', 'JJ'),\n",
              " ('people', 'NNS'),\n",
              " ('outside', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('company', 'NN'),\n",
              " ('took', 'VBD'),\n",
              " ('him', 'PRP'),\n",
              " ('seriously', 'RB'),\n",
              " ('.', '.'),\n",
              " ('I', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('tell', 'VB'),\n",
              " ('you', 'PRP'),\n",
              " ('very', 'RB'),\n",
              " ('senior', 'JJ'),\n",
              " ('CEOs', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('major', 'JJ'),\n",
              " ('American', 'JJ'),\n",
              " ('car', 'NN'),\n",
              " ('companies', 'NNS'),\n",
              " ('would', 'MD'),\n",
              " ('shake', 'VB'),\n",
              " ('my', 'PRP$'),\n",
              " ('hand', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('turn', 'VB'),\n",
              " ('away', 'RB'),\n",
              " ('because', 'IN'),\n",
              " ('I', 'PRP'),\n",
              " ('wasn', 'VBP'),\n",
              " ('’', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('worth', 'NN'),\n",
              " ('talking', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " (',', ','),\n",
              " ('said', 'VBD'),\n",
              " ('Thrun', 'NNP'),\n",
              " (',', ','),\n",
              " ('in', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('interview', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('Recode', 'NNP'),\n",
              " ('earlier', 'RBR'),\n",
              " ('this', 'DT'),\n",
              " ('week', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy\n",
        "\n",
        "# Tagging the tokens\n",
        "for token in doc:\n",
        "    print(token, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sin8HHZK1ARz",
        "outputId": "e3e9c23c-137c-4a0f-a7bd-fdfbdbb0a0e1"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When SCONJ\n",
            "Sebastian ADJ\n",
            "Thrun PROPN\n",
            "started VERB\n",
            "working VERB\n",
            "on ADP\n",
            "self NOUN\n",
            "- PUNCT\n",
            "driving VERB\n",
            "cars NOUN\n",
            "at ADP\n",
            "Google PROPN\n",
            "in ADP\n",
            "2007 NUM\n",
            ", PUNCT\n",
            "few ADJ\n",
            "people NOUN\n",
            "outside ADP\n",
            "of ADP\n",
            "the DET\n",
            "company NOUN\n",
            "took VERB\n",
            "him PRON\n",
            "seriously ADV\n",
            ". PUNCT\n",
            "I PRON\n",
            "can AUX\n",
            "tell VERB\n",
            "you PRON\n",
            "very ADV\n",
            "senior ADJ\n",
            "CEOs NOUN\n",
            "of ADP\n",
            "major ADJ\n",
            "American ADJ\n",
            "car NOUN\n",
            "companies NOUN\n",
            "would AUX\n",
            "shake VERB\n",
            "my PRON\n",
            "hand NOUN\n",
            "and CCONJ\n",
            "turn VERB\n",
            "away ADV\n",
            "because SCONJ\n",
            "I PRON\n",
            "was AUX\n",
            "n’t PART\n",
            "worth ADJ\n",
            "talking VERB\n",
            "to ADP\n",
            ", PUNCT\n",
            "said VERB\n",
            "Thrun PROPN\n",
            ", PUNCT\n",
            "in ADP\n",
            "an DET\n",
            "interview NOUN\n",
            "with ADP\n",
            "Recode PROPN\n",
            "earlier ADV\n",
            "this DET\n",
            "week NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, each token gets tagged into its respective part of speech. Let's understand a few of these tags:\n",
        "\n",
        "VERB - verbs (all tenses and modes)\n",
        "\n",
        "NOUN — nouns (common and proper)\n",
        "\n",
        "PRON — pronouns\n",
        "\n",
        "ADJ — adjectives\n",
        "\n",
        "ADV — adverbs\n",
        "\n",
        "ADP — adpositions (prepositions and postpositions)\n",
        "\n",
        "CONJ — conjunctions\n",
        "\n",
        "The other tags can also be understood by referring to this article. https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html"
      ],
      "metadata": {
        "id": "da0ljlRm13Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You want list of Verb tokens\n",
        "print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO7JyYNX2NAl",
        "outputId": "d81d7180-6cee-4953-cb85-8c6e60097094"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verbs: ['started', 'working', 'driving', 'took', 'tell', 'shake', 'turn', 'talking', 'said']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You want list of Noun tokens\n",
        "print(\"Nouns:\", [token.text for token in doc if token.pos_ == \"NOUN\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_uDInXs2yd8",
        "outputId": "73ac38aa-bef3-4b0c-dd7d-ec5bb69f03ad"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns: ['self', 'cars', 'people', 'company', 'CEOs', 'car', 'companies', 'hand', 'interview', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We had successfully tagged every word token present in the data. Now let's implement the collocations.**"
      ],
      "metadata": {
        "id": "zpfeu3EN3Qpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "Named Entity Recognition (NER) is an NLP-based technique to identify mentions of rigid designators from the text belonging to particular semantic types such as a person, location, organization etc.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "TYPE\tDescription\n",
        "LOC\t Non-GPE locations, mountain ranges, bodies of water\n",
        "bold    textMONEY\tMonetary values, including unit.\n",
        "ORG\t Companies, agencies, institutions etc.\n",
        "PERSON  People, including fictional\n",
        "NORP\tNationalities or religious or political groups\n",
        "FAC\tBuildings, airports, highways, bridge, etc.\n",
        "GPE\tCountries, cities, states.\n",
        "LANGUAGE\tAny named language\n",
        "PRODUCT\tObjects, vehicles, foods,etc. (Not services)\n",
        "LAW\tNamed documents made into laws\n",
        "EVENT\tNamed hurricanes, battles, wars, sports events,etc.\n",
        "DATE\tAbsolute or relative dates or periods\n",
        "TIME\tTimes smaller than a day\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We observe from the above table that there are several types of Named Entities with their own descriptions.\n",
        "\n",
        "Now, let's implement the same using both the NLTK and spaCy libraries."
      ],
      "metadata": {
        "id": "6dMq_9QS3iwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK\n",
        "\n",
        "# Using ne_chunk() method available in NLTK, we can recognize named entities using a classifier, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n",
        "# First it will tokenize the whole corpus, and tag into its respective pos tags. In final step, each entity get identified into their respective entity tags.\n",
        "\n",
        "ne_tree = ne_chunk(nltk.pos_tag(word_tokenize(text)))\n",
        "\n",
        "# Displaying the results\n",
        "print(ne_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXCE-Q583_-o",
        "outputId": "59aab564-023a-4c8b-e8a3-22906632346c"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  When/WRB\n",
            "  (PERSON Sebastian/JJ Thrun/NNP)\n",
            "  started/VBD\n",
            "  working/VBG\n",
            "  on/IN\n",
            "  self-driving/JJ\n",
            "  cars/NNS\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  in/IN\n",
            "  2007/CD\n",
            "  ,/,\n",
            "  few/JJ\n",
            "  people/NNS\n",
            "  outside/IN\n",
            "  of/IN\n",
            "  the/DT\n",
            "  company/NN\n",
            "  took/VBD\n",
            "  him/PRP\n",
            "  seriously/RB\n",
            "  ./.\n",
            "  I/PRP\n",
            "  can/MD\n",
            "  tell/VB\n",
            "  you/PRP\n",
            "  very/RB\n",
            "  senior/JJ\n",
            "  (ORGANIZATION CEOs/NNP)\n",
            "  of/IN\n",
            "  major/JJ\n",
            "  (GPE American/JJ)\n",
            "  car/NN\n",
            "  companies/NNS\n",
            "  would/MD\n",
            "  shake/VB\n",
            "  my/PRP$\n",
            "  hand/NN\n",
            "  and/CC\n",
            "  turn/VB\n",
            "  away/RB\n",
            "  because/IN\n",
            "  I/PRP\n",
            "  wasn/VBP\n",
            "  ’/JJ\n",
            "  t/NN\n",
            "  worth/NN\n",
            "  talking/VBG\n",
            "  to/TO\n",
            "  ,/,\n",
            "  said/VBD\n",
            "  (PERSON Thrun/NNP)\n",
            "  ,/,\n",
            "  in/IN\n",
            "  an/DT\n",
            "  interview/NN\n",
            "  with/IN\n",
            "  (PERSON Recode/NNP)\n",
            "  earlier/RBR\n",
            "  this/DT\n",
            "  week/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We observe that each word present in the data is tagged to its respective named entity."
      ],
      "metadata": {
        "id": "xlu1wcEY4GNz"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy\n",
        "\n",
        "print(\"Named entities in the given text are\\n\")\n",
        "\n",
        "# doc is corpus, doc.ents gives available entites in that corpus\n",
        "\n",
        "for ent in doc.ents: \n",
        "\n",
        "    # Printing the entity text and its label\n",
        "    print(ent.text,ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjPJ2WUq4RgO",
        "outputId": "4f351b8e-12c1-405b-e6a2-dff2ff3fd4f1"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities in the given text are\n",
            "\n",
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   We can infer that Sebastian Thrun is a name of the PERSON, 2007 is a DATE, American is a Nationality, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_JZ87s4daz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Comparison\n",
        "Finally, let's take a quantitative final look at the performance / speed of both libraries.\n",
        "\n",
        "For instance, let us look at the Word Tokenization operation for simiplicity."
      ],
      "metadata": {
        "id": "xmEECrr44qZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Tokenization on NLTK vs spaCy**"
      ],
      "metadata": {
        "id": "IgWnN2c04zzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NLTK\n",
        "Nw = %timeit -o nltk.tokenize.word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnSydKr74tbw",
        "outputId": "f9dae8bb-60ef-497b-ec88-71037c9d72f8"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "473 µs ± 132 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy\n",
        "Sw = %timeit -o nlp(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1523L7x4yxL",
        "outputId": "936b25c7-a2a2-4651-b56b-0fc31e389bc2"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.5 µs ± 6.28 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's clear to see that the spaCy library takes much less computational time than NLTK to perform the same task on the same corpus.\n",
        "\n",
        "This is quite true across the board, and spaCy is hence preferred in performance & deployment contexts for creating finalized NLP business solutions. NLTK, on the other hand, is preferred at the research & prototyping stage when coming up with Proof-of-Concepts, due to the lack of necessity of performance / speed at that stage and also its comprehensive features for all major NLP operations."
      ],
      "metadata": {
        "id": "0NkVQ8K85MnN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlNYZAjT5RYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}